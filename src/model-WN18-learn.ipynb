{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 18:34:59.615591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from scipy.stats import rankdata\n",
    "import math\n",
    "# import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 18:35:02.320173: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-20 18:35:02.322456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-06-20 18:35:02.359400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:02.359720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2022-06-20 18:35:02.359777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-06-20 18:35:02.384810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-06-20 18:35:02.384920: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-06-20 18:35:02.399391: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-06-20 18:35:02.403399: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-06-20 18:35:02.427420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-06-20 18:35:02.431959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-06-20 18:35:02.473336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-06-20 18:35:02.473466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:02.473678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:02.473788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-06-20 18:35:02.474360: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-20 18:35:02.475008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:02.475148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2022-06-20 18:35:02.475169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-06-20 18:35:02.475188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-06-20 18:35:02.475200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-06-20 18:35:02.475210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-06-20 18:35:02.475221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-06-20 18:35:02.475231: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-06-20 18:35:02.475242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-06-20 18:35:02.475253: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-06-20 18:35:02.475293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:02.475435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:02.475541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-06-20 18:35:02.475832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-06-20 18:35:03.351254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-20 18:35:03.351282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-06-20 18:35:03.351292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-06-20 18:35:03.351766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:03.352097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:03.352339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-20 18:35:03.352566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5146 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2022-06-20 18:35:03.354173: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "train = np.load('../data/WN18_numpy/train.npy')\n",
    "validation = np.load('../data/WN18_numpy/validation.npy')\n",
    "test = np.load('../data/WN18_numpy/test.npy')\n",
    "\n",
    "entities = np.load('../data/WN18_numpy/entities.npy')\n",
    "relations = np.load('../data/WN18_numpy/relations.npy')\n",
    "\n",
    "train = tf.convert_to_tensor(train.astype(dtype=np.int64), dtype=tf.int64)\n",
    "validation = tf.convert_to_tensor(validation.astype(dtype=np.int64), dtype=tf.int64)\n",
    "test = tf.convert_to_tensor(test.astype(dtype=np.int64), dtype=tf.int64)\n",
    "relations = tf.convert_to_tensor(relations.astype(dtype=np.float64), dtype=tf.float64)\n",
    "entities = tf.convert_to_tensor(entities.astype(dtype=np.float64), dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_embedings = True\n",
    "_read_Last_state = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = tf.constant(10)\n",
    "dim = tf.constant(100)\n",
    "kns = tf.constant(1)\n",
    "kns_r = tf.constant(1)\n",
    "alpha = tf.Variable(0.006, dtype = tf.float64)\n",
    "beta = tf.Variable(0.006, dtype = tf.float64)\n",
    "z = tf.constant(5, dtype = tf.float64)\n",
    "if _read_Last_state:\n",
    "      nn0_numpy = np.load('../data/WN18_numpy/nn0.npy')\n",
    "      nn1_numpy = np.load('../data/WN18_numpy/nn1.npy')\n",
    "      nn2_numpy = np.load('../data/WN18_numpy/nn2.npy')\n",
    "      nn2_numpy = np.load('../data/WN18_numpy/nn2.npy')\n",
    "      nn0 = tf.convert_to_tensor(nn0_numpy.astype(dtype=np.float64), dtype=tf.float64)\n",
    "      nn1 = tf.convert_to_tensor(nn1_numpy.astype(dtype=np.float64), dtype=tf.float64)\n",
    "      nn2 = tf.convert_to_tensor(nn2_numpy.astype(dtype=np.float64), dtype=tf.float64)\n",
    "      nn0_numpy = None\n",
    "      nn1_numpy = None\n",
    "      nn2_numpy = None\n",
    "      startIndex= np.load('../data/WN18_numpy/x.npy')\n",
    "else:\n",
    "      nn1 = tf.random.uniform( shape=(entities.shape[0], dim),\n",
    "            # minval=tf.math.truediv(z , tf.cast(dim,tf.float64),'minval'), \n",
    "            maxval=tf.math.truediv(z , tf.cast(dim,tf.float64),'maxval'),\n",
    "            dtype=tf.dtypes.float64, seed=1, name='nn1')\n",
    "            \n",
    "      nn0 =tf.random.uniform(shape=(entities.shape[0], dim), \n",
    "            # minval=tf.math.truediv(z , tf.cast(dim,tf.float64),'minval'), \n",
    "            maxval=tf.math.truediv(z , tf.cast(dim,tf.float64),'maxval'),\n",
    "            dtype=tf.dtypes.float64, seed=1, name='nn0')\n",
    "      nn2 = tf.random.uniform(shape=(relations.shape[0], dim), \n",
    "            # minval=tf.math.truediv(z , tf.cast(dim,tf.float64),'minval'), \n",
    "            maxval=tf.math.truediv(z , tf.cast(dim,tf.float64),'maxval'),\n",
    "            dtype=tf.dtypes.float64, seed=1, name='nn2')     \n",
    "      startIndex= 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def calculateRank(triple):  \n",
    "    start = time.time() \n",
    "    selectedEntities = tf.cast(tf.gather(entities, 0, axis=1), dtype=tf.int64)\n",
    "    _head_index = tf.gather(triple,0)\n",
    "    _tail_index = tf.gather(triple,1)\n",
    "    _relation_index = tf.gather(triple,2)  \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(train, 1, axis=1) == _tail_index) , (tf.gather(train, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(train, indexes), 0, axis=1)\n",
    "    shape = tf.constant([entities.shape[0]], dtype=tf.int64)   \n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTrain = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "\n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(test, 1, axis=1) == _tail_index) , (tf.gather(test, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(test, indexes), 0, axis=1)\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTest = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(validation, 1, axis=1) == _tail_index) , (tf.gather(validation, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(validation, indexes), 0, axis=1)\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesvalidation = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "\n",
    "    selectedEntitiesTrain= tf.reshape(selectedEntitiesTrain, (1, selectedEntitiesTrain.shape[0]))\n",
    "    selectedEntitiesTest = tf.reshape(selectedEntitiesTest, (1, selectedEntitiesTest.shape[0]))\n",
    "    selectedEntitiesvalidation = tf.reshape(selectedEntitiesvalidation, (1, selectedEntitiesvalidation.shape[0]))\n",
    "    selectedEntitiesFinal = tf.sets.intersection(selectedEntitiesTrain, selectedEntitiesTest)\n",
    "    selectedEntitiesFinal = tf.cast(tf.sets.intersection(selectedEntitiesFinal, selectedEntitiesvalidation), dtype=tf.int64).values\n",
    "    couraptedH = tf.concat([  tf.reshape(selectedEntitiesFinal, (selectedEntitiesFinal.shape[0], 1)) , tf.fill([selectedEntitiesFinal.shape[0], 1], _tail_index) , tf.fill([selectedEntitiesFinal.shape[0], 1], _relation_index)],1)\n",
    "    couraptedH = tf.concat([couraptedH, tf.reshape(triple, (1,3))], 0)\n",
    "    indexes = None\n",
    "    existEntites = None\n",
    "    selectedEntitiesTrain = None\n",
    "    selectedEntitiesTest = None\n",
    "    selectedEntitiesvalidation = None\n",
    "    selectedEntitiesFinal = None\n",
    "    div = tf.constant(3)\n",
    "    sliceSize = tf.cast(tf.math.floor(tf.math.truediv(couraptedH.shape[0], div)),dtype=tf.int64)\n",
    "    s0, s1, s2= tf.split(couraptedH, num_or_size_splits=[\n",
    "        sliceSize , \n",
    "        sliceSize , \n",
    "        tf.math.subtract(couraptedH.shape[0] , tf.math.multiply(sliceSize, 2)) ], axis=0)\n",
    "    couraptedH = None\n",
    "    # p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "\n",
    "    p = tf.math.subtract(1 , tf.concat([p0,p1,p2,], 0))\n",
    "    ranks = rankdata(p.numpy(), method='dense')\n",
    "    # ss = tf.sort(p0, axis=-1, direction='ASCENDING', name=None)\n",
    "    # file1 = open('../data/test.txt', \"w\")\n",
    "    # for x in ss:\n",
    "    #     file1.writelines(str(tf.get_static_value(x)))\n",
    "    #     file1.writelines('\\n')\n",
    "    # file1.close()\n",
    "    rankH = ranks[-1]\n",
    "    p0 = None\n",
    "    p1 = None\n",
    "    p2 = None\n",
    "    p = None\n",
    "\n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(train, 0, axis=1) == _head_index) , (tf.gather(train, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(train, indexes), 1, axis=1)\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTrain = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(test, 0, axis=1) == _head_index) , (tf.gather(test, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(test, indexes), 1, axis=1)\n",
    "\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTest = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(validation, 0, axis=1) == _head_index) , (tf.gather(validation, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(validation, indexes), 1, axis=1)\n",
    "\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesvalidation = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    selectedEntitiesTrain= tf.reshape(selectedEntitiesTrain, (1, selectedEntitiesTrain.shape[0]))\n",
    "    selectedEntitiesTest = tf.reshape(selectedEntitiesTest, (1, selectedEntitiesTest.shape[0]))\n",
    "    selectedEntitiesvalidation = tf.reshape(selectedEntitiesvalidation, (1, selectedEntitiesvalidation.shape[0]))\n",
    "\n",
    "    selectedEntitiesFinal = tf.sets.intersection(selectedEntitiesTrain, selectedEntitiesTest)\n",
    "    selectedEntitiesFinal = tf.cast(tf.sets.intersection(selectedEntitiesFinal, selectedEntitiesvalidation), dtype=tf.int64).values\n",
    "    couraptedT = tf.concat([   tf.fill([selectedEntitiesFinal.shape[0], 1], _head_index) , tf.reshape(selectedEntitiesFinal, (selectedEntitiesFinal.shape[0], 1)) , tf.fill([selectedEntitiesFinal.shape[0], 1], _relation_index)],1)\n",
    "    couraptedT = tf.concat([couraptedT, tf.reshape(triple, (1,3))], 0)    \n",
    "    indexes = None\n",
    "    existEntites = None\n",
    "    selectedEntitiesTrain = None\n",
    "    selectedEntitiesTest = None\n",
    "    selectedEntitiesvalidation = None\n",
    "    selectedEntitiesFinal = None\n",
    "    div = tf.constant(3)\n",
    "    sliceSize = tf.cast(tf.math.floor(tf.math.truediv(couraptedT.shape[0], div)),dtype=tf.int64)\n",
    "    s0, s1, s2= tf.split(couraptedT, num_or_size_splits=[\n",
    "        sliceSize , \n",
    "        sliceSize , \n",
    "        tf.math.subtract(couraptedT.shape[0] , tf.math.multiply(sliceSize, 2)) ], axis=0)\n",
    "    couraptedT = None\n",
    "    # p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p = tf.math.subtract(1 , tf.concat([p0,p1,p2,], 0))\n",
    "    ranks = rankdata(p.numpy(), method='dense')    \n",
    "    rankT = ranks[-1]\n",
    "    # rankT = tf.where(tf.math.top_k( p ,k = p.shape[0]).indices  == tf.subtract(p.shape[0] , 1) )\n",
    "    p = None\n",
    "    # print(rankH, rankT)\n",
    "    if tf.math.greater(rankH , rankT):\n",
    "        return rankT\n",
    "        # return tf.math.add(rankT,1)\n",
    "    else:\n",
    "        return rankH\n",
    "        # return tf.math.add(rankH,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(ranks):\n",
    "    inverse = []\n",
    "    one = tf.constant(1.0, dtype = tf.float64)\n",
    "    for rank in ranks:\n",
    "        inverse.append( tf.math.truediv(one , tf.cast(rank,tf.float64)))\n",
    "    summ = tf.reduce_sum(inverse)\n",
    "    return tf.math.multiply(tf.math.truediv(one , len(inverse)) , summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampleling_tail(nn0, nn1, nn2):\n",
    "    paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    t_row = 50000#train.shape[0]\n",
    "    with tf.device('/cpu:0'):\n",
    "        total_samples  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "    for tIndex in range(0 , t_row):\n",
    "        triple = tf.gather(train, tIndex)\n",
    "        _head_index = tf.gather(triple,0)\n",
    "        _relation_index = tf.gather(triple,2)\n",
    "        _vHead = tf.gather(nn0,_head_index)\n",
    "        _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "        samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "        samples = tf.pad(samples, paddings, constant_values=0)\n",
    "        samples =  tf.cast(samples, tf.int64)         \n",
    "        samples = tf.transpose(tf.concat([samples, [[tf.gather(triple,1)],[1]]], 1))\n",
    "        indices = tf.gather(samples, 0, axis=1)\n",
    "        _nn1_samples = tf.gather(nn1, indices)\n",
    "        _sigmoid =tf.math.sigmoid(tf.tensordot( _nn1_samples , tf.transpose(tf.math.add(_vHead , _vRel)) , axes=1))\n",
    "        cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "        g = tf.math.multiply(alpha , cost)\n",
    "        g1 = tf.math.multiply(beta , cost)\n",
    "        _nn1_samples = tf.math.add(tf.math.multiply(tf.gather(nn1, indices) , tf.reshape(g, (tf.math.add(kns,1), 1))) , _nn1_samples)\n",
    "        nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims(indices, 1),_nn1_samples)\n",
    "        _nn2_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn1_samples, axis=0, keepdims=False) , _vRel) , tf.reshape(g1, (g1.shape[0], 1))), axis=0, keepdims=False)  , _vRel)\n",
    "        indices = tf.constant(_relation_index)\n",
    "        nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims([indices], 1), tf.reshape(_nn2_sample,(1, _nn2_sample.shape[0])))\n",
    "\n",
    "        indices = tf.constant(_head_index)\n",
    "        _nn0_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn1_samples, axis=0, keepdims=False) , _vHead) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vHead)\n",
    "        nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims([indices], 1), tf.reshape(_nn0_sample,(1, _nn0_sample.shape[0])))\n",
    "    return nn0, nn1, nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampleling_head(nn0, nn1, nn2):\n",
    "    paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    t_row = 50000#train.shape[0]\n",
    "    with tf.device('/cpu:0'):\n",
    "        total_samples  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "        total_samples_r  = tf.gather( tf.random.categorical([tf.gather(relations, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "    for tIndex in range(0 , t_row):\n",
    "        triple = tf.gather(train, tIndex)\n",
    "        _head_index = tf.gather(triple,0)\n",
    "        _tail_index = tf.gather(triple,1)\n",
    "        _relation_index = tf.gather(triple,2)\n",
    "        \n",
    "        _vHead = tf.gather(nn0,_head_index)\n",
    "        _vTail = tf.gather(nn1,_tail_index)\n",
    "        _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "        samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "        samples = tf.pad(samples, paddings, constant_values=0)\n",
    "        samples =  tf.cast(samples, tf.int64)         \n",
    "        samples = tf.transpose(tf.concat([samples, [[_head_index],[1]]], 1))\n",
    "        indices = tf.gather(samples, 0, axis=1)\n",
    "        _nn0_samples = tf.gather(nn0, indices)\n",
    "\n",
    "        samples_r = [tf.slice(total_samples_r,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "        samples_r = tf.pad(samples_r, paddings, constant_values=0)\n",
    "        samples_r = tf.cast(samples_r, tf.int64)\n",
    "        samples_r = tf.transpose(tf.concat([samples_r, [[_relation_index],[1]]], 1))\n",
    "        indices_r = tf.gather(samples_r, 0, axis=1)\n",
    "        _nn2_sample = tf.gather(nn2, indices_r)\n",
    "\n",
    "\n",
    "        _sigmoid =tf.math.sigmoid(tf.tensordot( _nn0_samples , tf.transpose(tf.math.add(_vTail , _nn2_sample)) , axes=1))\n",
    "        cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "        g = tf.linalg.diag_part(tf.math.multiply(alpha , cost))\n",
    "        g1 = tf.linalg.diag_part(tf.math.multiply(beta , cost))\n",
    "\n",
    "        _nn0_samples = tf.math.add(tf.math.multiply(tf.gather(nn0, indices) , tf.reshape(g, (tf.math.add(kns,1), 1))) , _nn0_samples)\n",
    "        nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims(indices, 1),_nn0_samples)\n",
    "\n",
    "        _nn2_sample = tf.math.add(tf.math.multiply(tf.math.add(_nn0_samples , _nn2_sample) , tf.reshape(g1, (g1.shape[0], 1)))  , _nn2_sample)\n",
    "        nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims(indices_r, 1),_nn2_sample)\n",
    "\n",
    "        indices = tf.constant(_tail_index)\n",
    "        _nn1_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn0_samples, axis=0, keepdims=False) , _vTail) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vTail)\n",
    "        nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims([indices], 1), tf.reshape(_nn1_sample,(1, _nn1_sample.shape[0])))\n",
    "    # paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    # t_row = train.shape[0]\n",
    "    # with tf.device('/cpu:0'):\n",
    "    #     total_samples  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "    # for tIndex in range(0 , t_row):\n",
    "    #     triple = tf.gather(train, tIndex)\n",
    "    #     _head_index = tf.gather(triple,0)\n",
    "    #     _tail_index = tf.gather(triple,1)\n",
    "    #     _relation_index = tf.gather(triple,2)\n",
    "        \n",
    "    #     _vHead = tf.gather(nn0,_head_index)\n",
    "    #     _vTail = tf.gather(nn1,_tail_index)\n",
    "    #     _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "    #     samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "    #     samples = tf.pad(samples, paddings, constant_values=0)\n",
    "    #     samples =  tf.cast(samples, tf.int64)         \n",
    "    #     samples = tf.transpose(tf.concat([samples, [[_head_index],[1]]], 1))\n",
    "    #     indices = tf.gather(samples, 0, axis=1)\n",
    "    #     _nn0_samples = tf.gather(nn0, indices)\n",
    "    #     _sigmoid =tf.math.sigmoid(tf.tensordot( _nn0_samples , tf.transpose(tf.math.add(_vTail , _vRel)) , axes=1))\n",
    "    #     cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "    #     g = tf.math.multiply(alpha , cost)\n",
    "    #     g1 = tf.math.multiply(beta , cost)\n",
    "\n",
    "    #     _nn0_samples = tf.math.add(tf.math.multiply(tf.gather(nn0, indices) , tf.reshape(g, (tf.math.add(kns,1), 1))) , _nn0_samples)\n",
    "    #     nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims(indices, 1),_nn0_samples)\n",
    "\n",
    "    #     _nn2_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn0_samples, axis=0, keepdims=False) , _vRel) , tf.reshape(g1, (g1.shape[0], 1))), axis=0, keepdims=False)  , _vRel)\n",
    "    #     indices = tf.constant(_relation_index)\n",
    "    #     nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims([indices], 1), tf.reshape(_nn2_sample,(1, _nn2_sample.shape[0])))\n",
    "\n",
    "    #     indices = tf.constant(_tail_index)\n",
    "    #     _nn1_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn0_samples, axis=0, keepdims=False) , _vTail) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vTail)\n",
    "    #     nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims([indices], 1), tf.reshape(_nn1_sample,(1, _nn1_sample.shape[0])))\n",
    "    return nn0, nn1, nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampleling_rel(nn0, nn1, nn2):\n",
    "    paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    t_row = 50000#train.shape[0]\n",
    "    with tf.device('/cpu:0'):\n",
    "        total_samples  = tf.gather( tf.random.categorical([tf.gather(relations, 2, axis=1)], tf.math.multiply(kns_r,t_row) , dtype=None, seed=None),0)\n",
    "        total_samples_t  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns_r,t_row) , dtype=None, seed=None),0)\n",
    "    for tIndex in range(0 , t_row):\n",
    "        triple = tf.gather(train, tIndex)\n",
    "        _head_index = tf.gather(triple,0)\n",
    "        _tail_index = tf.gather(triple,1)\n",
    "        _relation_index = tf.gather(triple,2)\n",
    "        \n",
    "        _vHead = tf.gather(nn0,_head_index)\n",
    "        _vTail = tf.gather(nn1,_tail_index)\n",
    "        _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "        samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns_r)],size=[kns_r])]\n",
    "        samples = tf.pad(samples, paddings, constant_values=0)\n",
    "        samples =  tf.cast(samples, tf.int64)         \n",
    "        samples = tf.transpose(tf.concat([samples, [[_relation_index],[1]]], 1))\n",
    "        indices = tf.gather(samples, 0, axis=1)\n",
    "        _nn2_samples = tf.gather(nn2, indices)\n",
    "\n",
    "        samples_t = [tf.slice(total_samples_t,begin=[tf.math.multiply(tIndex, kns_r)],size=[kns_r])]\n",
    "        samples_t = tf.pad(samples_t, paddings, constant_values=0)\n",
    "        samples_t = tf.cast(samples_t, tf.int64)\n",
    "        samples_t = tf.transpose(tf.concat([samples_t, [[_tail_index],[1]]], 1))\n",
    "        indices_t = tf.gather(samples_t, 0, axis=1)\n",
    "        _nn1_sample = tf.gather(nn1, indices_t)\n",
    "\n",
    "        _sigmoid =tf.math.sigmoid(tf.tensordot( _nn2_samples , tf.transpose(tf.math.add(_vTail , _vHead)) , axes=1))\n",
    "        cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "        g = tf.math.multiply(alpha , cost)\n",
    "        g1 = tf.math.multiply(beta , cost)\n",
    "\n",
    "        _nn2_samples = tf.math.add(tf.math.multiply(tf.gather(nn2, indices) , tf.reshape(g1, (tf.math.add(kns_r,1), 1))) , _nn2_samples)\n",
    "        nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims(indices, 1),_nn2_samples)\n",
    "\n",
    "        _nn1_sample = tf.math.add(tf.math.multiply(tf.math.add(_nn2_samples , _nn1_sample) , tf.reshape(g1, (g.shape[0], 1)))  , _nn1_sample)\n",
    "        nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims(indices_t, 1),_nn1_sample)\n",
    "\n",
    "        indices = tf.constant(_head_index)\n",
    "        _nn0_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn1_sample, axis=0, keepdims=False) , _vHead) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vHead)\n",
    "        nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims([indices], 1), tf.reshape(_nn0_sample,(1, _nn0_sample.shape[0])))\n",
    "    return nn0, nn1, nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(startIndex , iteration):\n",
    "    train = tf.random.shuffle(train, seed=None, name=None)\n",
    "    if x%3 ==0 :\n",
    "        nn0, nn1, nn2 = negative_sampleling_tail(nn0, nn1, nn2)\n",
    "    elif x%3==1:\n",
    "        nn0, nn1, nn2 = negative_sampleling_rel(nn0, nn1, nn2)\n",
    "    else:\n",
    "        nn0, nn1, nn2 = negative_sampleling_head(nn0, nn1, nn2)\n",
    "    # nn0, nn1, nn2 = negative_sampleling_tail(nn0, nn1, nn2)   \n",
    "\n",
    "    if _save_embedings:\n",
    "        np.save('../data/WN18_numpy/nn0.npy', nn0.numpy())\n",
    "        np.save('../data/WN18_numpy/nn1.npy', nn1.numpy())\n",
    "        np.save('../data/WN18_numpy/nn2.npy', nn2.numpy())\n",
    "        np.save('../data/WN18_numpy/x.npy',x)\n",
    "    # if x % 5 == 0 :\n",
    "    #     print(\"Iteration ==> \", x) \n",
    "    if tf.math.reduce_all(tf.math.is_nan(nn0)) or tf.math.reduce_all(tf.math.is_nan(nn1)) or tf.math.reduce_all(tf.math.is_nan(nn2)):\n",
    "        print(\"fill with nan\") \n",
    "        break\n",
    "    # if x % 3 ==0:\n",
    "    #     tensor = []\n",
    "    #     # for tIndex in range(0 , 20):\n",
    "    #     for tIndex in range(0 ,100):\n",
    "    #         triple = tf.gather(validation,tIndex)\n",
    "    #         tensor.append(calculateRank(triple))\n",
    "    #     print(\"MRR at iteration %3d ==> %10f\" %(x, mrr(tensor)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = []\n",
    "# for tIndex in range(0 ,test.shape[0]):\n",
    "#         triple = tf.gather(test,tIndex)\n",
    "#         tensor.append(calculateRank(triple))\n",
    "# print(\"TEST *** MRR at iteration %3d ==> %10f\" %(x, mrr(tensor)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array(tensor)\n",
    "# # a = a[500:]\n",
    "# a.shape\n",
    "# np.save('../data/WN18_numpy/rank_test.npy',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([18493 19195     4], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "triple = tf.gather(train, 0)\n",
    "print(triple)\n",
    "\n",
    "_head_index = tf.gather(triple,0)\n",
    "_tail_index = tf.gather(triple,1)\n",
    "_relation_index = tf.gather(triple,2)\n",
    "\n",
    "_vHead = tf.gather(nn0,_head_index)\n",
    "_vTail = tf.gather(nn1,_tail_index)\n",
    "_vRel = tf.gather(nn2,_relation_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([35703 35702     7], shape=(3,), dtype=int64)\n",
      "tf.Tensor(0.9628267884254456, shape=(), dtype=float64)\n",
      "15.671381356771242\n",
      "35.41955091967329\n",
      "35.48972995651894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "triple = tf.gather(test, 202)\n",
    "print(triple)\n",
    "\n",
    "_head_index = tf.gather(triple,0)\n",
    "_tail_index = tf.gather(triple,1)\n",
    "_relation_index = tf.gather(triple,2)\n",
    "\n",
    "_vHead = tf.gather(nn0,_head_index)\n",
    "_vTail = tf.gather(nn1,_tail_index)\n",
    "_vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "print(cosine_loss([_vHead ],[_vRel ]))\n",
    "print(math.degrees(math.acos(cosine_loss([_vHead ],[_vRel ]))))\n",
    "print(math.degrees(math.acos(cosine_loss([_vTail ],[_vRel ]))))\n",
    "print(math.degrees(math.acos(cosine_loss([_vHead + _vRel ],[ _vTail]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "for tIndex in range(0 ,test.shape[0]):\n",
    "    triple = tf.gather(test, tIndex)\n",
    "    _head_index = tf.gather(triple,0)\n",
    "    _tail_index = tf.gather(triple,1)\n",
    "    _relation_index = tf.gather(triple,2)\n",
    "\n",
    "    _vHead = tf.gather(nn0,_head_index)\n",
    "    _vTail = tf.gather(nn1,_tail_index)\n",
    "    _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "    if dic.get(_relation_index.numpy()) == None:\n",
    "        dic[_relation_index.numpy()] = []\n",
    "    dic.get(_relation_index.numpy()).append(\n",
    "        {'hr':math.degrees(math.acos(cosine_loss([_vHead ],[_vRel ]))) , \n",
    "        'tr':math.degrees(math.acos(cosine_loss([_vTail ],[_vRel ]))), \n",
    "        'm':math.degrees(math.acos(cosine_loss([_vHead + _vRel ],[ _vTail])))\n",
    "        \n",
    "        })      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = []\n",
    "for x in dic.get(9):\n",
    "    hr.append(x['hr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n, bins, patches = plt.hist(hr, 70, density=True, facecolor='r', alpha=1)\n",
    "\n",
    "\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "# plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.xlim(0, 180)\n",
    "# plt.ylim(0, 0.03)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f3807b41f1dd5b2dd3ef97aff1934d1fc74c65dc41f378f0e5f9b4b6f88fa84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
