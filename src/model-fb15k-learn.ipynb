{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:35:31.944021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from scipy.stats import rankdata\n",
    "# import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 17:35:33.755874: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-16 17:35:33.757155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-06-16 17:35:33.780982: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-06-16 17:35:33.781017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: sogol\n",
      "2022-06-16 17:35:33.781024: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: sogol\n",
      "2022-06-16 17:35:33.781121: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-06-16 17:35:33.781144: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-06-16 17:35:33.781150: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-06-16 17:35:33.781545: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-16 17:35:33.782011: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "train = np.load('../data/freebase15k_numpy/train.npy')\n",
    "validation = np.load('../data/freebase15k_numpy/validation.npy')\n",
    "test = np.load('../data/freebase15k_numpy/test.npy')\n",
    "\n",
    "entities = np.load('../data/freebase15k_numpy/entities.npy')\n",
    "relations = np.load('../data/freebase15k_numpy/relations.npy')\n",
    "\n",
    "train = tf.convert_to_tensor(train.astype(dtype=np.int64), dtype=tf.int64)\n",
    "validation = tf.convert_to_tensor(validation.astype(dtype=np.int64), dtype=tf.int64)\n",
    "test = tf.convert_to_tensor(test.astype(dtype=np.int64), dtype=tf.int64)\n",
    "relations = tf.convert_to_tensor(relations.astype(dtype=np.float64), dtype=tf.float64)\n",
    "entities = tf.convert_to_tensor(entities.astype(dtype=np.float64), dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_embedings = False\n",
    "_read_Last_state = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = tf.constant(15)\n",
    "dim = tf.constant(100)\n",
    "kns = tf.constant(5)\n",
    "kns_r = tf.constant(5)\n",
    "alpha = tf.Variable(0.005, dtype = tf.float64)\n",
    "beta = tf.Variable(0.005, dtype = tf.float64)\n",
    "z = tf.constant(5, dtype = tf.float64)\n",
    "if _read_Last_state:\n",
    "      nn0_numpy = np.load('../data/freebase15k_numpy/nn0.npy')\n",
    "      nn1_numpy = np.load('../data/freebase15k_numpy/nn1.npy')\n",
    "      nn2_numpy = np.load('../data/freebase15k_numpy/nn2.npy')\n",
    "      nn2_numpy = np.load('../data/freebase15k_numpy/nn2.npy')\n",
    "      nn0 = tf.convert_to_tensor(nn0_numpy.astype(dtype=np.float64), dtype=tf.float64)\n",
    "      nn1 = tf.convert_to_tensor(nn1_numpy.astype(dtype=np.float64), dtype=tf.float64)\n",
    "      nn2 = tf.convert_to_tensor(nn2_numpy.astype(dtype=np.float64), dtype=tf.float64)\n",
    "      nn0_numpy = None\n",
    "      nn1_numpy = None\n",
    "      nn2_numpy = None\n",
    "      startIndex= np.load('../data/freebase15k_numpy/x.npy')\n",
    "else:\n",
    "      nn1 = tf.random.uniform( shape=(entities.shape[0], dim),\n",
    "            # minval=tf.math.truediv(z , tf.cast(dim,tf.float64),'minval'), \n",
    "            maxval=tf.math.truediv(z , tf.cast(dim,tf.float64),'maxval'),\n",
    "            dtype=tf.dtypes.float64, seed=1, name='nn1')\n",
    "            \n",
    "      nn0 =tf.random.uniform(shape=(entities.shape[0], dim), \n",
    "            # minval=tf.math.truediv(z , tf.cast(dim,tf.float64),'minval'), \n",
    "            maxval=tf.math.truediv(z , tf.cast(dim,tf.float64),'maxval'),\n",
    "            dtype=tf.dtypes.float64, seed=1, name='nn0')\n",
    "      nn2 = tf.random.uniform(shape=(relations.shape[0], dim), \n",
    "            # minval=tf.math.truediv(z , tf.cast(dim,tf.float64),'minval'), \n",
    "            maxval=tf.math.truediv(z , tf.cast(dim,tf.float64),'maxval'),\n",
    "            dtype=tf.dtypes.float64, seed=1, name='nn2')     \n",
    "      startIndex= 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def calculateRank(triple):  \n",
    "    start = time.time() \n",
    "    selectedEntities = tf.cast(tf.gather(entities, 0, axis=1), dtype=tf.int64)\n",
    "    _head_index = tf.gather(triple,0)\n",
    "    _tail_index = tf.gather(triple,1)\n",
    "    _relation_index = tf.gather(triple,2)  \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(train, 1, axis=1) == _tail_index) , (tf.gather(train, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(train, indexes), 0, axis=1)\n",
    "    shape = tf.constant([entities.shape[0]], dtype=tf.int64)   \n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTrain = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "\n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(test, 1, axis=1) == _tail_index) , (tf.gather(test, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(test, indexes), 0, axis=1)\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTest = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(validation, 1, axis=1) == _tail_index) , (tf.gather(validation, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(validation, indexes), 0, axis=1)\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesvalidation = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "\n",
    "    selectedEntitiesTrain= tf.reshape(selectedEntitiesTrain, (1, selectedEntitiesTrain.shape[0]))\n",
    "    selectedEntitiesTest = tf.reshape(selectedEntitiesTest, (1, selectedEntitiesTest.shape[0]))\n",
    "    selectedEntitiesvalidation = tf.reshape(selectedEntitiesvalidation, (1, selectedEntitiesvalidation.shape[0]))\n",
    "    selectedEntitiesFinal = tf.sets.intersection(selectedEntitiesTrain, selectedEntitiesTest)\n",
    "    selectedEntitiesFinal = tf.cast(tf.sets.intersection(selectedEntitiesFinal, selectedEntitiesvalidation), dtype=tf.int64).values\n",
    "    couraptedH = tf.concat([  tf.reshape(selectedEntitiesFinal, (selectedEntitiesFinal.shape[0], 1)) , tf.fill([selectedEntitiesFinal.shape[0], 1], _tail_index) , tf.fill([selectedEntitiesFinal.shape[0], 1], _relation_index)],1)\n",
    "    couraptedH = tf.concat([couraptedH, tf.reshape(triple, (1,3))], 0)\n",
    "    indexes = None\n",
    "    existEntites = None\n",
    "    selectedEntitiesTrain = None\n",
    "    selectedEntitiesTest = None\n",
    "    selectedEntitiesvalidation = None\n",
    "    selectedEntitiesFinal = None\n",
    "    div = tf.constant(3)\n",
    "    sliceSize = tf.cast(tf.math.floor(tf.math.truediv(couraptedH.shape[0], div)),dtype=tf.int64)\n",
    "    s0, s1, s2= tf.split(couraptedH, num_or_size_splits=[\n",
    "        sliceSize , \n",
    "        sliceSize , \n",
    "        tf.math.subtract(couraptedH.shape[0] , tf.math.multiply(sliceSize, 2)) ], axis=0)\n",
    "    couraptedH = None\n",
    "    # p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "\n",
    "    p = tf.math.subtract(1 , tf.concat([p0,p1,p2,], 0))\n",
    "    ranks = rankdata(p.numpy(), method='dense')\n",
    "    # ss = tf.sort(p0, axis=-1, direction='ASCENDING', name=None)\n",
    "    # file1 = open('../data/test.txt', \"w\")\n",
    "    # for x in ss:\n",
    "    #     file1.writelines(str(tf.get_static_value(x)))\n",
    "    #     file1.writelines('\\n')\n",
    "    # file1.close()\n",
    "    rankH = ranks[-1]\n",
    "    p0 = None\n",
    "    p1 = None\n",
    "    p2 = None\n",
    "    p = None\n",
    "\n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(train, 0, axis=1) == _head_index) , (tf.gather(train, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(train, indexes), 1, axis=1)\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTrain = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(test, 0, axis=1) == _head_index) , (tf.gather(test, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(test, indexes), 1, axis=1)\n",
    "\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesTest = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    indexes = tf.where( tf.math.logical_and( (tf.gather(validation, 0, axis=1) == _head_index) , (tf.gather(validation, 2, axis=1) == _relation_index)))\n",
    "    indexes = tf.reshape(indexes, (indexes.shape[0] ))\n",
    "    existEntites = tf.gather(tf.gather(validation, indexes), 1, axis=1)\n",
    "\n",
    "    scatter = tf.scatter_nd(tf.reshape(existEntites ,  (existEntites.shape[0] , 1) ), tf.math.add(existEntites ,1), shape)\n",
    "    selectedEntitiesvalidation = tf.where(tf.math.greater(tf.subtract(selectedEntities , scatter), -1)) \n",
    "    selectedEntitiesTrain= tf.reshape(selectedEntitiesTrain, (1, selectedEntitiesTrain.shape[0]))\n",
    "    selectedEntitiesTest = tf.reshape(selectedEntitiesTest, (1, selectedEntitiesTest.shape[0]))\n",
    "    selectedEntitiesvalidation = tf.reshape(selectedEntitiesvalidation, (1, selectedEntitiesvalidation.shape[0]))\n",
    "\n",
    "    selectedEntitiesFinal = tf.sets.intersection(selectedEntitiesTrain, selectedEntitiesTest)\n",
    "    selectedEntitiesFinal = tf.cast(tf.sets.intersection(selectedEntitiesFinal, selectedEntitiesvalidation), dtype=tf.int64).values\n",
    "    couraptedT = tf.concat([   tf.fill([selectedEntitiesFinal.shape[0], 1], _head_index) , tf.reshape(selectedEntitiesFinal, (selectedEntitiesFinal.shape[0], 1)) , tf.fill([selectedEntitiesFinal.shape[0], 1], _relation_index)],1)\n",
    "    couraptedT = tf.concat([couraptedT, tf.reshape(triple, (1,3))], 0)    \n",
    "    indexes = None\n",
    "    existEntites = None\n",
    "    selectedEntitiesTrain = None\n",
    "    selectedEntitiesTest = None\n",
    "    selectedEntitiesvalidation = None\n",
    "    selectedEntitiesFinal = None\n",
    "    div = tf.constant(3)\n",
    "    sliceSize = tf.cast(tf.math.floor(tf.math.truediv(couraptedT.shape[0], div)),dtype=tf.int64)\n",
    "    s0, s1, s2= tf.split(couraptedT, num_or_size_splits=[\n",
    "        sliceSize , \n",
    "        sliceSize , \n",
    "        tf.math.subtract(couraptedT.shape[0] , tf.math.multiply(sliceSize, 2)) ], axis=0)\n",
    "    couraptedT = None\n",
    "    # p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    # p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p0 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s0,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s0,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p1 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s1,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s1,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn0,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn1,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\n",
    "    p = tf.math.subtract(1 , tf.concat([p0,p1,p2,], 0))\n",
    "    ranks = rankdata(p.numpy(), method='dense')    \n",
    "    rankT = ranks[-1]\n",
    "    # rankT = tf.where(tf.math.top_k( p ,k = p.shape[0]).indices  == tf.subtract(p.shape[0] , 1) )\n",
    "    p = None\n",
    "    # print(rankH, rankT)\n",
    "    if tf.math.greater(rankH , rankT):\n",
    "        return rankT\n",
    "        # return tf.math.add(rankT,1)\n",
    "    else:\n",
    "        return rankH\n",
    "        # return tf.math.add(rankH,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(ranks):\n",
    "    inverse = []\n",
    "    one = tf.constant(1.0, dtype = tf.float64)\n",
    "    for rank in ranks:\n",
    "        inverse.append( tf.math.truediv(one , tf.cast(rank,tf.float64)))\n",
    "    summ = tf.reduce_sum(inverse)\n",
    "    return tf.math.multiply(tf.math.truediv(one , len(inverse)) , summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HitAtK(ranks, k=10):\n",
    "    _hitAt = 0\n",
    "    for rank in ranks:\n",
    "        if rank <= k:\n",
    "            _hitAt+=1\n",
    "    return _hitAt / len(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr(ranks):\n",
    "    _mean = 0\n",
    "    one = tf.constant(1.0, dtype = tf.float64)\n",
    "    for rank in ranks:\n",
    "        _mean + = rank\n",
    "    return _mean / len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampleling_tail(nn0, nn1, nn2):\n",
    "    paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    t_row = train.shape[0]\n",
    "    with tf.device('/cpu:0'):\n",
    "        total_samples  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "    for tIndex in range(0 , t_row):\n",
    "        triple = tf.gather(train, tIndex)\n",
    "        _head_index = tf.gather(triple,0)\n",
    "        _relation_index = tf.gather(triple,2)\n",
    "        _vHead = tf.gather(nn0,_head_index)\n",
    "        _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "        samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "        samples = tf.pad(samples, paddings, constant_values=0)\n",
    "        samples =  tf.cast(samples, tf.int64)         \n",
    "        samples = tf.transpose(tf.concat([samples, [[tf.gather(triple,1)],[1]]], 1))\n",
    "        indices = tf.gather(samples, 0, axis=1)\n",
    "        _nn1_samples = tf.gather(nn1, indices)\n",
    "        _sigmoid =tf.math.sigmoid(tf.tensordot( _nn1_samples , tf.transpose(tf.math.add(_vHead , _vRel)) , axes=1))\n",
    "        cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "        g = tf.math.multiply(alpha , cost)\n",
    "        g1 = tf.math.multiply(beta , cost)\n",
    "        _nn1_samples = tf.math.add(tf.math.multiply(tf.gather(nn1, indices) , tf.reshape(g, (tf.math.add(kns,1), 1))) , _nn1_samples)\n",
    "        nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims(indices, 1),_nn1_samples)\n",
    "        _nn2_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn1_samples, axis=0, keepdims=False) , _vRel) , tf.reshape(g1, (g1.shape[0], 1))), axis=0, keepdims=False)  , _vRel)\n",
    "        indices = tf.constant(_relation_index)\n",
    "        nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims([indices], 1), tf.reshape(_nn2_sample,(1, _nn2_sample.shape[0])))\n",
    "\n",
    "        indices = tf.constant(_head_index)\n",
    "        _nn0_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn1_samples, axis=0, keepdims=False) , _vHead) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vHead)\n",
    "        nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims([indices], 1), tf.reshape(_nn0_sample,(1, _nn0_sample.shape[0])))\n",
    "    return nn0, nn1, nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampleling_head(nn0, nn1, nn2):\n",
    "    paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    t_row = train.shape[0]\n",
    "    with tf.device('/cpu:0'):\n",
    "        total_samples  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "        total_samples_r  = tf.gather( tf.random.categorical([tf.gather(relations, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "    for tIndex in range(0 , t_row):\n",
    "        triple = tf.gather(train, tIndex)\n",
    "        _head_index = tf.gather(triple,0)\n",
    "        _tail_index = tf.gather(triple,1)\n",
    "        _relation_index = tf.gather(triple,2)\n",
    "        \n",
    "        _vHead = tf.gather(nn0,_head_index)\n",
    "        _vTail = tf.gather(nn1,_tail_index)\n",
    "        _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "        samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "        samples = tf.pad(samples, paddings, constant_values=0)\n",
    "        samples =  tf.cast(samples, tf.int64)         \n",
    "        samples = tf.transpose(tf.concat([samples, [[_head_index],[1]]], 1))\n",
    "        indices = tf.gather(samples, 0, axis=1)\n",
    "        _nn0_samples = tf.gather(nn0, indices)\n",
    "\n",
    "        samples_r = [tf.slice(total_samples_r,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "        samples_r = tf.pad(samples_r, paddings, constant_values=0)\n",
    "        samples_r = tf.cast(samples_r, tf.int64)\n",
    "        samples_r = tf.transpose(tf.concat([samples_r, [[_relation_index],[1]]], 1))\n",
    "        indices_r = tf.gather(samples_r, 0, axis=1)\n",
    "        _nn2_sample = tf.gather(nn2, indices_r)\n",
    "\n",
    "\n",
    "        _sigmoid =tf.math.sigmoid(tf.tensordot( _nn0_samples , tf.transpose(tf.math.add(_vTail , _nn2_sample)) , axes=1))\n",
    "        cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "        g = tf.linalg.diag_part(tf.math.multiply(alpha , cost))\n",
    "        g1 = tf.linalg.diag_part(tf.math.multiply(beta , cost))\n",
    "\n",
    "        _nn0_samples = tf.math.add(tf.math.multiply(tf.gather(nn0, indices) , tf.reshape(g, (tf.math.add(kns,1), 1))) , _nn0_samples)\n",
    "        nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims(indices, 1),_nn0_samples)\n",
    "\n",
    "        _nn2_sample = tf.math.add(tf.math.multiply(tf.math.add(_nn0_samples , _nn2_sample) , tf.reshape(g1, (g1.shape[0], 1)))  , _nn2_sample)\n",
    "        nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims(indices_r, 1),_nn2_sample)\n",
    "\n",
    "        indices = tf.constant(_tail_index)\n",
    "        _nn1_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn0_samples, axis=0, keepdims=False) , _vTail) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vTail)\n",
    "        nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims([indices], 1), tf.reshape(_nn1_sample,(1, _nn1_sample.shape[0])))\n",
    "    # paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    # t_row = train.shape[0]\n",
    "    # with tf.device('/cpu:0'):\n",
    "    #     total_samples  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns,t_row) , dtype=None, seed=None),0)\n",
    "    # for tIndex in range(0 , t_row):\n",
    "    #     triple = tf.gather(train, tIndex)\n",
    "    #     _head_index = tf.gather(triple,0)\n",
    "    #     _tail_index = tf.gather(triple,1)\n",
    "    #     _relation_index = tf.gather(triple,2)\n",
    "        \n",
    "    #     _vHead = tf.gather(nn0,_head_index)\n",
    "    #     _vTail = tf.gather(nn1,_tail_index)\n",
    "    #     _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "    #     samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns)],size=[kns])]\n",
    "    #     samples = tf.pad(samples, paddings, constant_values=0)\n",
    "    #     samples =  tf.cast(samples, tf.int64)         \n",
    "    #     samples = tf.transpose(tf.concat([samples, [[_head_index],[1]]], 1))\n",
    "    #     indices = tf.gather(samples, 0, axis=1)\n",
    "    #     _nn0_samples = tf.gather(nn0, indices)\n",
    "    #     _sigmoid =tf.math.sigmoid(tf.tensordot( _nn0_samples , tf.transpose(tf.math.add(_vTail , _vRel)) , axes=1))\n",
    "    #     cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "    #     g = tf.math.multiply(alpha , cost)\n",
    "    #     g1 = tf.math.multiply(beta , cost)\n",
    "\n",
    "    #     _nn0_samples = tf.math.add(tf.math.multiply(tf.gather(nn0, indices) , tf.reshape(g, (tf.math.add(kns,1), 1))) , _nn0_samples)\n",
    "    #     nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims(indices, 1),_nn0_samples)\n",
    "\n",
    "    #     _nn2_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn0_samples, axis=0, keepdims=False) , _vRel) , tf.reshape(g1, (g1.shape[0], 1))), axis=0, keepdims=False)  , _vRel)\n",
    "    #     indices = tf.constant(_relation_index)\n",
    "    #     nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims([indices], 1), tf.reshape(_nn2_sample,(1, _nn2_sample.shape[0])))\n",
    "\n",
    "    #     indices = tf.constant(_tail_index)\n",
    "    #     _nn1_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn0_samples, axis=0, keepdims=False) , _vTail) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vTail)\n",
    "    #     nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims([indices], 1), tf.reshape(_nn1_sample,(1, _nn1_sample.shape[0])))\n",
    "    return nn0, nn1, nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampleling_rel(nn0, nn1, nn2):\n",
    "    paddings = tf.constant([[0, 1,], [0, 0]])\n",
    "    t_row = train.shape[0]\n",
    "    with tf.device('/cpu:0'):\n",
    "        total_samples  = tf.gather( tf.random.categorical([tf.gather(relations, 2, axis=1)], tf.math.multiply(kns_r,t_row) , dtype=None, seed=None),0)\n",
    "        total_samples_t  = tf.gather( tf.random.categorical([tf.gather(entities, 2, axis=1)], tf.math.multiply(kns_r,t_row) , dtype=None, seed=None),0)\n",
    "    for tIndex in range(0 , t_row):\n",
    "        triple = tf.gather(train, tIndex)\n",
    "        _head_index = tf.gather(triple,0)\n",
    "        _tail_index = tf.gather(triple,1)\n",
    "        _relation_index = tf.gather(triple,2)\n",
    "        \n",
    "        _vHead = tf.gather(nn0,_head_index)\n",
    "        _vTail = tf.gather(nn1,_tail_index)\n",
    "        _vRel = tf.gather(nn2,_relation_index)\n",
    "\n",
    "        samples = [tf.slice(total_samples,begin=[tf.math.multiply(tIndex, kns_r)],size=[kns_r])]\n",
    "        samples = tf.pad(samples, paddings, constant_values=0)\n",
    "        samples =  tf.cast(samples, tf.int64)         \n",
    "        samples = tf.transpose(tf.concat([samples, [[_relation_index],[1]]], 1))\n",
    "        indices = tf.gather(samples, 0, axis=1)\n",
    "        _nn2_samples = tf.gather(nn2, indices)\n",
    "\n",
    "        samples_t = [tf.slice(total_samples_t,begin=[tf.math.multiply(tIndex, kns_r)],size=[kns_r])]\n",
    "        samples_t = tf.pad(samples_t, paddings, constant_values=0)\n",
    "        samples_t = tf.cast(samples_t, tf.int64)\n",
    "        samples_t = tf.transpose(tf.concat([samples_t, [[_tail_index],[1]]], 1))\n",
    "        indices_t = tf.gather(samples_t, 0, axis=1)\n",
    "        _nn1_sample = tf.gather(nn1, indices_t)\n",
    "\n",
    "        _sigmoid =tf.math.sigmoid(tf.tensordot( _nn2_samples , tf.transpose(tf.math.add(_vTail , _vHead)) , axes=1))\n",
    "        cost = tf.math.subtract(tf.cast(tf.gather(samples, 1, axis=1), tf.float64) , _sigmoid)\n",
    "        g = tf.math.multiply(alpha , cost)\n",
    "        g1 = tf.math.multiply(beta , cost)\n",
    "\n",
    "        _nn2_samples = tf.math.add(tf.math.multiply(tf.gather(nn2, indices) , tf.reshape(g1, (tf.math.add(kns_r,1), 1))) , _nn2_samples)\n",
    "        nn2 = tf.tensor_scatter_nd_update(nn2,tf.expand_dims(indices, 1),_nn2_samples)\n",
    "\n",
    "        _nn1_sample = tf.math.add(tf.math.multiply(tf.math.add(_nn2_samples , _nn1_sample) , tf.reshape(g1, (g.shape[0], 1)))  , _nn1_sample)\n",
    "        nn1 = tf.tensor_scatter_nd_update(nn1,tf.expand_dims(indices_t, 1),_nn1_sample)\n",
    "\n",
    "        indices = tf.constant(_head_index)\n",
    "        _nn0_sample = tf.math.add(tf.math.reduce_sum(tf.math.multiply(tf.math.add(tf.math.reduce_sum(_nn1_sample, axis=0, keepdims=False) , _vHead) , tf.reshape(g, (g.shape[0], 1))), axis=0, keepdims=False)  , _vHead)\n",
    "        nn0 = tf.tensor_scatter_nd_update(nn0,tf.expand_dims([indices], 1), tf.reshape(_nn0_sample,(1, _nn0_sample.shape[0])))\n",
    "    return nn0, nn1, nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25762/1258671160.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_sampleling_tail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_sampleling_rel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25762/363708173.py\u001b[0m in \u001b[0;36mnegative_sampleling_tail\u001b[0;34m(nn0, nn1, nn2)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0m_nn2_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_nn1_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_vRel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0m_vRel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_relation_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_scatter_nd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_nn2_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nn2_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_head_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[1;32m    195\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(startIndex , iteration):\n",
    "    train = tf.random.shuffle(train, seed=None, name=None)\n",
    "    if x%3 ==0 :\n",
    "        nn0, nn1, nn2 = negative_sampleling_tail(nn0, nn1, nn2)\n",
    "    elif x%3==1:\n",
    "        nn0, nn1, nn2 = negative_sampleling_rel(nn0, nn1, nn2)\n",
    "    else:\n",
    "        nn0, nn1, nn2 = negative_sampleling_head(nn0, nn1, nn2)\n",
    "    # nn0, nn1, nn2 = negative_sampleling_head(nn0, nn1, nn2)   \n",
    "\n",
    "    if x > 10 and _save_embedings:\n",
    "        np.save('../data/freebase15k_numpy/nn0.npy', nn0.numpy())\n",
    "        np.save('../data/freebase15k_numpy/nn1.npy', nn1.numpy())\n",
    "        np.save('../data/freebase15k_numpy/nn2.npy', nn2.numpy())\n",
    "        np.save('../data/freebase15k_numpy/x.npy',x)\n",
    "    # if x % 5 == 0 :\n",
    "    #     print(\"Iteration ==> \", x) \n",
    "    if tf.math.reduce_all(tf.math.is_nan(nn0)) or tf.math.reduce_all(tf.math.is_nan(nn1)) or tf.math.reduce_all(tf.math.is_nan(nn2)):\n",
    "        print(\"fill with nan\") \n",
    "        break\n",
    "    if x %3 ==0:\n",
    "        tensor = []\n",
    "        # for tIndex in range(0 , 20):\n",
    "        for tIndex in range(0 ,500):\n",
    "            triple = tf.gather(validation,tIndex)\n",
    "            tensor.append(calculateRank(triple))\n",
    "        print(\"MRR at iteration %3d ==> %10f\" %(x, mrr(tensor)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25762/4040743037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtIndex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtriple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"TEST *** MRR at iteration %3d ==> %10f\" %(x, mrr(tensor)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25762/3259803618.py\u001b[0m in \u001b[0;36mcalculateRank\u001b[0;34m(triple)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# p2 =  tf.linalg.diag_part(tf.math.sigmoid(tf.tensordot(  tf.gather(nn1,tf.gather(s2,1, axis=1),0),tf.transpose(  tf.gather( nn0,  tf.gather( s2,0, axis=1)) + tf.gather(nn2,_relation_index) ),axes=1 )))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mp0\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_relation_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_relation_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnn1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_relation_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensor = []\n",
    "for tIndex in range(0 ,150):\n",
    "        triple = tf.gather(test,tIndex)\n",
    "        tensor.append(calculateRank(triple))\n",
    "# print(\"TEST *** MRR at iteration %3d ==> %10f\" %(x, mrr(tensor)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16354\n",
      "21378\n",
      "9987\n",
      "11352\n"
     ]
    }
   ],
   "source": [
    "_1_to_1 = []\n",
    "_1_to_N = []\n",
    "_N_to_1 = []\n",
    "_N_to_N = []\n",
    "\n",
    "relations_entity_idx = tf.constant(7)\n",
    "for tIndex in range(0 ,test.shape[0]):\n",
    "    triple = tf.gather(test,tIndex)\n",
    "    _Head = tf.gather(entities,tf.gather(triple,0))\n",
    "    _Tail = tf.gather(entities,tf.gather(triple,1))\n",
    "    _relaion = tf.gather(relations,tf.gather(triple,2))\n",
    "    if tf.math.equal(tf.gather(_relaion,relations_entity_idx),1):\n",
    "        _1_to_1.append(1)\n",
    "    elif tf.math.equal(tf.gather(_relaion,relations_entity_idx),2):\n",
    "        _1_to_N.append(1)\n",
    "    elif tf.math.equal(tf.gather(_relaion,relations_entity_idx),3):\n",
    "         _N_to_1.append(1)\n",
    "    else:\n",
    "        _N_to_N.append(1)   \n",
    "print(len(_1_to_N))\n",
    "print(len(_N_to_1))\n",
    "print(len(_1_to_1))  \n",
    "print(len(_N_to_N))     "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f3807b41f1dd5b2dd3ef97aff1934d1fc74c65dc41f378f0e5f9b4b6f88fa84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
